{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/python3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/python3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/python3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/python3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/python3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/python3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/python3/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/python3/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/python3/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/python3/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/python3/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/python3/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM, GRU\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils, plot_model\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.models import load_model\n",
    "from tensorflow import nn as nn\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "with open('/home/tanyixin/ISWC_2020/data/all_defintion_pos.txt', 'r') as f:\n",
    "    corpus_chars = f.readlines()\n",
    "    \n",
    "corpus = []\n",
    "for line in corpus_chars:\n",
    "    text = line.strip().split(' ')[1:]\n",
    "    for word in text:\n",
    "        corpus.append(word)\n",
    "        \n",
    "corpus_index = list(set(corpus))\n",
    "corpus_index.append('b')\n",
    "vocab_size = len(corpus_index)\n",
    "print(vocab_size) # 16\n",
    "\n",
    "pos_index = {}\n",
    "for pos in range(len(corpus_index)):\n",
    "    pos_index[corpus_index[pos]] = pos\n",
    "    \n",
    "y_onehot = np_utils.to_categorical(np.array(range(len(corpus_index)), dtype='float64'))\n",
    "vector_onehot = (y_onehot*0.98)+0.01\n",
    "\n",
    "with open('/home/tanyixin/ISWC_2020/data/train_labels.txt', 'r') as f:\n",
    "    line_list = f.readlines()\n",
    "    \n",
    "manual_tags_train = {}\n",
    "for line in line_list:\n",
    "    line = line.strip().split(' ')\n",
    "    tag_name = line[0]\n",
    "    tag_pos = line[1]\n",
    "    manual_tags_train[tag_name] = tag_pos\n",
    "    \n",
    "with open('/home/tanyixin/ISWC_2020/data/candidate_nouns.txt', 'r') as f:\n",
    "    line_list = f.readlines()\n",
    "    \n",
    "candidate_tags = {}\n",
    "for line in line_list:\n",
    "    line = line.strip().split(' ')\n",
    "    candidate_tags[line[0]] = line[1:]\n",
    "    \n",
    "with open('/home/tanyixin/ISWC_2020/data/centrality.txt', 'r') as f:\n",
    "    line_list = f.readlines()\n",
    "    \n",
    "centrality = {}\n",
    "for line in line_list:\n",
    "    line = line.strip().split(' ')\n",
    "    centrality[line[0]] = float(line[1])\n",
    "    \n",
    "with open('/home/tanyixin/ISWC_2020/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "with open('/home/tanyixin/ISWC_2020/data/train_definition_pos.txt', 'r') as f:\n",
    "    line_list = f.readlines()\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "print(len(line_list))\n",
    "\n",
    "window_size = 9\n",
    "for line in line_list:\n",
    "    line = line.strip().split(' ')\n",
    "    tag = line[0]\n",
    "    try:\n",
    "        manual_tags_train[tag]\n",
    "        candidate_tags[tag]\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    sentence = line[1:]\n",
    "    index_nn_list = []\n",
    "    for index in range(len(sentence)):\n",
    "        if sentence[index] == 'NN':\n",
    "            index_nn_list.append(index)\n",
    "#     print(index_nn_list)\n",
    "    score = []\n",
    "    input_list = []\n",
    "    \n",
    "    target_word_index = 0\n",
    "    target_word = manual_tags_train[tag].split(',')[-1]\n",
    "    for index in range(len(candidate_tags[tag])):\n",
    "        if candidate_tags[tag][index] == target_word:\n",
    "            target_word_index = index\n",
    "        \n",
    "    count = 0\n",
    "    for index_nn in index_nn_list:\n",
    "        if index_nn - window_size < 0:\n",
    "            prev_three_index = 0\n",
    "        else:\n",
    "            prev_three_index = index_nn - window_size\n",
    "            \n",
    "        if index_nn + window_size > len(sentence) - 1:\n",
    "            next_three_index = len(sentence) - 1\n",
    "        else:\n",
    "            next_three_index = index_nn + window_size\n",
    "            \n",
    "#         print(prev_three_index)\n",
    "#         print(next_three_index)\n",
    "        prev_list = sentence[prev_three_index: index_nn]\n",
    "        next_list = sentence[index_nn + 1: next_three_index + 1]\n",
    "        \n",
    "        if len(prev_list) < window_size:\n",
    "            prev_list.insert(0, 'b')\n",
    "            \n",
    "        if len(next_list) < window_size:\n",
    "            next_list.append('b')\n",
    "        \n",
    "#         print(prev_list)\n",
    "#         print(next_list)\n",
    "        prev_score = []\n",
    "        next_score = []\n",
    "        for i in range(window_size):\n",
    "            try:\n",
    "                prev_score.append(vector_onehot[pos_index[prev_list[i]]])\n",
    "            except:\n",
    "                prev_score.append(vector_onehot[pos_index['b']])\n",
    "                \n",
    "            try:\n",
    "                next_score.append(vector_onehot[pos_index[next_list[i]]])\n",
    "            except:\n",
    "                next_score.append(vector_onehot[pos_index['b']])\n",
    "              \n",
    "        x.append(np.vstack((np.array(prev_score), np.array(next_score))))\n",
    "        if count == target_word_index:\n",
    "            y.append([0.99, 0.01])\n",
    "        else:\n",
    "            y.append([0.01, 0.99])\n",
    "        count += 1\n",
    "        \n",
    "x_train = np.array(x)\n",
    "y_train = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572.4087631702423\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Bidirectional(GRU(32, activation='tanh'), input_shape=(window_size*2, vocab_size)))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(Dense(16))\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Dense(2))\n",
    "model1.add(Activation('softmax'))\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "model1.fit(x_train, y_train, batch_size=32, epochs=50, verbose=0)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save('/home/tanyixin/ISWC_2020/data/model/keras_ws9.h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/python3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/python3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/python3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/python3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/python3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/python3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/python3/lib/python3.5/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/python3/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = load_model('/home/tanyixin/ISWC_2020/data/model/keras_ws9.h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750\n"
     ]
    }
   ],
   "source": [
    "with open('/home/tanyixin/ISWC_2020/data/test_definition_pos.txt', 'r') as f:\n",
    "    line_list = f.readlines()\n",
    "\n",
    "print(len(line_list))\n",
    "fff = open('/home/tanyixin/ISWC_2020/data/predict/keras_ws9.txt', 'w')\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for line in line_list:\n",
    "    line = line.strip().split(' ')\n",
    "    tag = line[0]\n",
    "    try:\n",
    "        candidate_tags[tag]\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    sentence = line[1:]\n",
    "    index_nn_list = []\n",
    "    x_test = []\n",
    "    for index in range(len(sentence)):\n",
    "        if sentence[index] == 'NN':\n",
    "            index_nn_list.append(index)\n",
    "#     print(index_nn_list)\n",
    "    if len(index_nn_list) == 0:\n",
    "        continue\n",
    "    score = []\n",
    "    input_list = []\n",
    "        \n",
    "    count = 0\n",
    "    for index_nn in index_nn_list:\n",
    "        if index_nn - window_size < 0:\n",
    "            prev_three_index = 0\n",
    "        else:\n",
    "            prev_three_index = index_nn - window_size\n",
    "            \n",
    "        if index_nn + window_size > len(sentence) - 1:\n",
    "            next_three_index = len(sentence) - 1\n",
    "        else:\n",
    "            next_three_index = index_nn + window_size\n",
    "            \n",
    "#         print(prev_three_index)\n",
    "#         print(next_three_index)\n",
    "        prev_list = sentence[prev_three_index: index_nn]\n",
    "        next_list = sentence[index_nn + 1: next_three_index + 1]\n",
    "        \n",
    "        if len(prev_list) < window_size:\n",
    "            prev_list.insert(0, 'b')\n",
    "            \n",
    "        if len(next_list) < window_size:\n",
    "            next_list.append('b')\n",
    "        \n",
    "#         print(prev_list)\n",
    "#         print(next_list)\n",
    "        x_temp = []\n",
    "        prev_score = []\n",
    "        next_score = []\n",
    "        for i in range(window_size):\n",
    "            try:\n",
    "                prev_score.append(vector_onehot[pos_index[prev_list[i]]])\n",
    "            except:\n",
    "                prev_score.append(vector_onehot[pos_index['b']])\n",
    "                \n",
    "            try:\n",
    "                next_score.append(vector_onehot[pos_index[next_list[i]]])\n",
    "            except:\n",
    "                next_score.append(vector_onehot[pos_index['b']])\n",
    "            \n",
    "        x_temp.append(np.vstack((np.array(prev_score), np.array(next_score))))\n",
    "        x_test.append(np.array(x_temp).reshape(window_size*2, vocab_size))\n",
    "    x_test = np.array(x_test)\n",
    "    try:\n",
    "        y_test = model.predict(x_test)\n",
    "    except:\n",
    "        print(tag)\n",
    "    tag_index = y_test[:,0].argmax()\n",
    "    tag_predict = candidate_tags[tag][tag_index]\n",
    "    fff.write(tag + ' ' + tag_predict + '\\n')\n",
    "fff.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "0.9937694704049844\n",
      "0.2102803738317757\n",
      "0.32710280373831774\n"
     ]
    }
   ],
   "source": [
    "# refinement uion\n",
    "with open('/home/tanyixin/ISWC_2020/data/train_definition_pos.txt', 'r') as f:\n",
    "    line_list = f.readlines()\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "print(len(line_list))\n",
    "\n",
    "for line in line_list:\n",
    "    line = line.strip().split(' ')\n",
    "    tag = line[0]\n",
    "    try:\n",
    "        manual_tags_train[tag]\n",
    "        candidate_tags[tag]\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    sentence = line[1:]\n",
    "    index_nn_list = []\n",
    "    for index in range(len(sentence)):\n",
    "        if sentence[index] == 'NN':\n",
    "            index_nn_list.append(index)\n",
    "#     print(index_nn_list)\n",
    "    score = []\n",
    "    input_list = []\n",
    "    \n",
    "    target_word_index = 0\n",
    "    target_word = manual_tags_train[tag].split(',')[-1]\n",
    "    for index in range(len(candidate_tags[tag])):\n",
    "        if candidate_tags[tag][index] == target_word:\n",
    "            target_word_index = index\n",
    "        \n",
    "    count = 0\n",
    "    for index_nn in index_nn_list:\n",
    "        if index_nn - window_size < 0:\n",
    "            prev_three_index = 0\n",
    "        else:\n",
    "            prev_three_index = index_nn - window_size\n",
    "            \n",
    "        if index_nn + window_size > len(sentence) - 1:\n",
    "            next_three_index = len(sentence) - 1\n",
    "        else:\n",
    "            next_three_index = index_nn + window_size\n",
    "            \n",
    "#         print(prev_three_index)\n",
    "#         print(next_three_index)\n",
    "        prev_list = sentence[prev_three_index: index_nn]\n",
    "        next_list = sentence[index_nn + 1: next_three_index + 1]\n",
    "        \n",
    "        if len(prev_list) < window_size:\n",
    "            prev_list.insert(0, 'b')\n",
    "            \n",
    "        if len(next_list) < window_size:\n",
    "            next_list.append('b')\n",
    "        \n",
    "#         print(prev_list)\n",
    "#         print(next_list)\n",
    "        prev_score = []\n",
    "        next_score = []\n",
    "        for i in range(window_size):\n",
    "            try:\n",
    "                prev_score.append(vector_onehot[pos_index[prev_list[i]]])\n",
    "            except:\n",
    "                prev_score.append(vector_onehot[pos_index['b']])\n",
    "                \n",
    "            try:\n",
    "                next_score.append(vector_onehot[pos_index[next_list[i]]])\n",
    "            except:\n",
    "                next_score.append(vector_onehot[pos_index['b']])\n",
    "              \n",
    "        x = np.vstack((np.array(prev_score), np.array(next_score)))\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x_pred = model.predict(x)\n",
    "        location = index_nn / len(sentence)\n",
    "        candidate_nn = candidate_tags[tag][count]\n",
    "        centrality_measure = centrality[candidate_nn]\n",
    "        capitalized = 0\n",
    "        if candidate_nn.istitle():\n",
    "            capitalized = 1\n",
    "        \n",
    "        print(centrality_measure)\n",
    "        if count == target_word_index:\n",
    "            y.append([0.99, 0.01])\n",
    "        else:\n",
    "            y.append([0.01, 0.99])\n",
    "        count += 1\n",
    "        \n",
    "    break\n",
    "        \n",
    "x_train = np.array(x)\n",
    "y_train = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['DT', 'NN', 'VBN', 'IN', 'VBG', 'NN', 'IN', 'DT', 'NN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'programming'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_tags['java'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Java'.istitle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
